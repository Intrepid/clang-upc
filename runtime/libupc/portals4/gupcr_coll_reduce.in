/*===-- gupcr_coll_reduce.in - UPC Runtime Support Library ---------------===
|*
|*                     The LLVM Compiler Infrastructure
|*
|* Copyright 2012-2014, Intrepid Technology, Inc.  All rights reserved.
|* This file is distributed under a BSD-style Open Source License.
|* See LICENSE-INTREPID.TXT for details.
|*
|* This file is derived from the UPC Collectives reference implementation
|* developed by Michigan Technological University.
|*
|* Copyright 2004, Michigan Technological University.  All rights reserved.
|* This file is distributed under a BSD-style Open Source License.
|* See LICENSE-MTU.TXT for details.
|*
|*===---------------------------------------------------------------------===*/

#include <stdlib.h>
#include <upc.h>
#include <upc_collective.h>
#include <upc_coll.h>
#include "gupcr_config.h"
#include "gupcr_defs.h"
#include "gupcr_sup.h"
#include "gupcr_portals.h"
#include "gupcr_gmem.h"
#include "gupcr_utils.h"
#include "gupcr_coll_sup.h"

/*****************************************************************************/
/*                                                                           */
/*        UPC collective function library, reference implementation          */
/*                                                                           */
/*   Steve Seidel, Dept. of Computer Science, Michigan Technological Univ.   */
/*   steve@mtu.edu                                        March 1, 2004      */
/*                                                                           */
/*****************************************************************************/

/**
 * @file gupcr_coll_reduce.upc
 * GUPC Portals4 reduce collectives implementation.
 *
 * @addtogroup COLLECTIVES GUPCR Collectives Functions
 * @{
 */

/** Collectives reduce storage pointer */
gupcr_reduce_str_t gupcr_reduce_storage;

/**
 * Convert from UPC reduce to Portals atomic operation.
 *
 * @parm [in] op UPC reduce operation
 * @retval Portals atomic operation
*/
ptl_op_t
gupcr_portals_reduce_op (upc_op_t op)
{
  switch (op)
    {
    case UPC_ADD:
      return PTL_SUM;
    case UPC_MULT:
      return PTL_PROD;
    case UPC_AND:
      return PTL_BAND;
    case UPC_OR:
      return PTL_BOR;
    case UPC_XOR:
      return PTL_BXOR;
    case UPC_LOGAND:
      return PTL_LAND;
    case UPC_LOGOR:
      return PTL_LOR;
    case UPC_MIN:
      return PTL_MIN;
    case UPC_MAX:
      return PTL_MAX;
    default:
      gupcr_fatal_error ("cannot convert UPC reduce operation 0x%lx.", op);
    }
}

PREPROCESS_BEGIN
#ifdef _UPC_NONINT_T
/**
 * Collectives UPC_LOGAND function for float types
 *
 * Portals4 does not define logical AND atomic operations
 * and they will be executed as functions.
 */
  _UPC_RED_T
gupcr_coll_logand_GENERIC (_UPC_RED_T a, _UPC_RED_T b)
{
  return a && b;
}

/**
 * Collectives UPC_LOGOR function for float types
 *
 * Portals4 does not define logical OR atomic operations
 * and they will be executed as functions.
 */

_UPC_RED_T
gupcr_coll_logor_GENERIC (_UPC_RED_T a, _UPC_RED_T b)
{
  return a || b;
}
#endif /* _UPC_NONINT_T */

/**
 * Collectives reduce (_GENERIC) function
 *
 * The following steps are taken to calculate the reduced value:
 *
 * - Each thread reduces the values it has affinity to. Note that
 *   some of the threads might not participate in collectives reduce.
 * - A reduce tree is created out of the threads participating.
 * - All the parent threads signal their children that they are ready
 *   for the collectives reduce operation.
 * - All the children perform atomic portals reduce operations in the
 *   parent shared space. The reduced values are propagated to the
 *   top of the tree.
 * - Result is written to the specified destination.
 *
 * @param [in] dst Destination shared pointer
 * @param [in] src Source shared pointer
 * @param [in] op Collectives reduce operation
 * @param [in] nelems Number of elements
 * @param [in] blk_size Block size
 * @param [in] func Optional reduce function
 * @param [in] sync_mode Synchronization mode
 *
 */
void upc_all_reduce_GENERIC
  (shared void *dst,
   shared const void *src,
   upc_op_t op,
   size_t nelems,
   size_t blk_size,
   _UPC_RED_T (*func) (_UPC_RED_T, _UPC_RED_T), upc_flag_t sync_mode)
{
  int i, n_local, full_rows, last_row;
  int num_thr, tail_thr, extras, ph, src_thr, dst_thr, velems, start;

  _UPC_RED_T local_result = 0;
  _UPC_RED_T *l_src;

  if (!upc_coll_init_flag)
    upc_coll_init ();

  gupcr_trace (FC_COLL, "COLL ALL_REDUCE ENTER _UPC_RED_T %lu %lu",
	       (long unsigned) nelems, (long unsigned) blk_size);

  if (blk_size == 0)
    blk_size = nelems;

#ifdef _UPC_COLL_CHECK_ARGS
  upc_coll_err (dst, src, NULL, 0, sync_mode, blk_size, nelems, op, UPC_RED);
#endif

  /* Synchronize using barriers in the cases of MYSYNC and ALLSYNC.  */
  if (UPC_IN_MYSYNC & sync_mode || !(UPC_IN_NOSYNC & sync_mode))
    upc_barrier;

  /* Compute n_local, the number of elements local to this thread.  */
  n_local = 0;

  /* Also compute start, the starting index of src for each thread.  */

  src_thr = upc_threadof ((shared void *) src);
  dst_thr = upc_threadof ((shared void *) dst);
  ph = upc_phaseof ((shared void *) src);

  /* nelems plus the number of virtual elements in first row.  */
  velems = nelems + src_thr * blk_size + ph;

  /* Include virtual elements when computing number of local elements.  */
  full_rows = velems / (blk_size * THREADS);
  last_row = velems % (blk_size * THREADS);
  tail_thr = last_row / blk_size;

  /* Calculate number of participating threads.  */
  num_thr = (nelems + ph + blk_size - 1) / blk_size;
  if (num_thr > THREADS)
    num_thr = THREADS;

  gupcr_debug (FC_COLL,
	       "src_thr: %d tail_thr: %d ph: %d num_thr: %d full_rows: %d",
	       src_thr, tail_thr, ph, num_thr, full_rows);

  /* Calculate number of local elements.  */
  if (blk_size > 0)
    {
      if (MYTHREAD <= tail_thr)
	if (MYTHREAD == tail_thr)
	  extras = last_row % blk_size;
	else
	  extras = blk_size;
      else
	extras = 0;

      n_local = blk_size * full_rows + extras;

      /* Adjust the number of elements in this thread, if necessary.  */
      if (MYTHREAD < src_thr)
	n_local -= blk_size;
      else if (MYTHREAD == src_thr)
	n_local -= ph;
    }
  else
    {
      n_local = 0;
      if (src_thr == MYTHREAD)	/* Revise the number of local elements.  */
	n_local = nelems;
    }

  /* Starting index for this thread
     Note: start is sometimes negative because src is
     addressed here as if its block size is 1.  */

  if (blk_size > 0)
    if (MYTHREAD > src_thr)
      start = MYTHREAD - src_thr - ph * THREADS;
    else if (MYTHREAD < src_thr)
      start = (blk_size - ph) * THREADS + MYTHREAD - src_thr;
    else			/* This is the source thread.  */
      start = 0;
  else
    start = 0;

#ifdef _UPC_NONINT_T
  /* Logical operations on floating point types must execute as
     functions as Portals4 does not have support for them.  */
  switch (op)
    {
    case UPC_LOGAND:
      func = &gupcr_coll_logand_GENERIC;
      op = UPC_FUNC;
      break;
    case UPC_LOGOR:
      func = &gupcr_coll_logor_GENERIC;
      op = UPC_FUNC;
      break;
    }
#endif /* _UPC_NOINT_T */

  /* Reduce the elements local to this thread.  */

  if (n_local > 0)
    {
      int loop_cnt = n_local - 1;

      l_src = (_UPC_RED_T *) ((shared const _UPC_RED_T *) src + start);
      local_result = *l_src++;

      switch (op)
	{
	case UPC_ADD:
	  while (loop_cnt--)
	    local_result += *l_src++;
	  break;
	case UPC_MULT:
	  while (loop_cnt--)
	    local_result *= *l_src++;
	  break;
#ifndef _UPC_NONINT_T
	  /* Skip if not integral type, per spec 4.3.1.1
	     (See additional comments in upc_collective.c) */
	case UPC_AND:
	  while (loop_cnt--)
	    local_result &= *l_src++;
	  break;
	case UPC_OR:
	  while (loop_cnt--)
	    local_result |= *l_src++;
	  break;
	case UPC_XOR:
	  while (loop_cnt--)
	    local_result ^= *l_src++;
	  break;
	case UPC_LOGAND:
	  while (loop_cnt--)
	    local_result = local_result && *l_src++;
	  break;
	case UPC_LOGOR:
	  while (loop_cnt--)
	    local_result = local_result || *l_src++;
	  break;
#endif /* !_UPC_NOINT_T */
	case UPC_MIN:
	  while (loop_cnt--)
	    {
	      if (local_result > *l_src)
		local_result = *l_src;
	      ++l_src;
	    }
	  break;
	case UPC_MAX:
	  while (loop_cnt--)
	    {
	      if (local_result < *l_src)
		local_result = *l_src;
	      ++l_src;
	    }
	  break;
	case UPC_FUNC:
	  while (loop_cnt--)
	    local_result = func (local_result, *l_src++);
	  break;
	case UPC_NONCOMM_FUNC:
	  while (loop_cnt--)
	    local_result = func (local_result, *l_src++);
	  break;
	default:
	  gupcr_fatal_error ("bad UPC collectives reduce operator 0x%lx", op);
	}
    }

  /* Note: local_result is undefined if n_local == 0.
     Note: Only a proper subset of threads have a meaningful local_result.
     Note: dst might be a thread that does not have a local result.  */

  /* Global reduce on only participating threads.  */
  if (n_local)
    {
      /* Local pointer where reduced values are written too.  */
      _UPC_RED_T *t_result =
	(_UPC_RED_T *) & gupcr_reduce_storage[MYTHREAD].value[0];

      /* Initialize collectives reduce tree.  */
      gupcr_coll_tree_setup (dst_thr, src_thr, num_thr);

      /* Copy in local results into the area for reduce operation.
         NOTE: Not needed for the case of collective functions. However,
         this covers the case of only one thread.  */
      *t_result = local_result;

#ifdef GUPCR_USE_PORTALS4_TRIGGERED_OPS
/* Run reduce operation without triggered functions.  */
#undef GUPCR_USE_PORTALS4_TRIGGERED_OPS
#endif
#if GUPCR_USE_PORTALS4_TRIGGERED_OPS
      /* Note: In the case of UPC_FUNC and UPC_NONCOMM, it is not possible
         to use triggered operations on inner nodes. In that case, inner
         nodes must calculate reduced value by calling the specified
         function.  */
      if (gupcr_coll_child_cnt)
	{
	  if (IS_ROOT_THREAD)
	    {
	      /* ROOT THREAD */
	      /* Let children know that parent is ready.  */
	      for (i = 0; i < gupcr_coll_child_cnt; i++)
		{
		  size_t offset = upc_addrfield ((shared void *)
						 &(gupcr_reduce_storage
						   [MYTHREAD].signal));
		  gupcr_coll_put (gupcr_coll_child[i], offset, offset, 1);
		}
	      gupcr_coll_ack_wait (gupcr_coll_child_cnt);

	      /* Wait for children to report their values.  */
	      gupcr_coll_signal_wait (gupcr_coll_child_cnt);

	      /* Reduce local values with those of children if necessary.  */
	      if ((op == UPC_FUNC) || (op == UPC_NONCOMM_FUNC))
		{
		  /* Reduce local result with those of children.  */
		  for (i = 0; i < gupcr_coll_child_cnt; i++)
		    {
		      local_result =
			func (local_result, *(_UPC_RED_T *)
			      & gupcr_reduce_storage[MYTHREAD].value[i]);
		    }
		  *t_result = local_result;
		}
	    }
	  else
	    {
	      /* INNER THREAD */
	      /* Prepare triggered atomic function.  */
	      if ((op != UPC_FUNC) && (op != UPC_NONCOMM_FUNC))
		{
		  /* Use triggered atomic operations once children sent
		     their results and parent is ready to receive it.  */
		  size_t offset = upc_addrfield ((shared void *)
						 &(gupcr_reduce_storage
						   [MYTHREAD].value[0]));
		  gupcr_coll_trigput_atomic (gupcr_coll_parent_thread, offset,
					     offset, sizeof (_UPC_RED_T),
					     gupcr_portals_reduce_op (op),
					     _UPC_TO_PTL_TYPECVT,
					     gupcr_coll_child_cnt + 1);
		}
	      /* Let children know that parent is ready.  */
	      for (i = 0; i < gupcr_coll_child_cnt; i++)
		{
		  size_t offset = upc_addrfield ((shared void *)
						 &(gupcr_reduce_storage
						   [MYTHREAD].signal));
		  gupcr_coll_put (gupcr_coll_child[i], offset, offset, 1);
		}
	      gupcr_coll_ack_wait (gupcr_coll_child_cnt);

	      /* Wait for completion, children and parent are ready.  */
	      gupcr_coll_signal_wait (gupcr_coll_child_cnt + 1);
	      /* Execute reduce functions if necessary.  */
	      if ((op == UPC_FUNC) || (op == UPC_NONCOMM_FUNC))
		{
		  size_t offset = upc_addrfield ((shared void *)
						 &(gupcr_reduce_storage
						   [MYTHREAD].value[0]));
		  size_t doffset =
		    upc_addrfield ((shared void *)
				   &(gupcr_reduce_storage[MYTHREAD].value
				     [gupcr_coll_child_index]));
		  /* Reduce local result with those of children.  */
		  for (i = 0; i < gupcr_coll_child_cnt; i++)
		    {
		      local_result = func (local_result, *(_UPC_RED_T *)
					   &
					   gupcr_reduce_storage
					   [MYTHREAD].value[i]);
		    }
		  *t_result = local_result;
		  gupcr_coll_put (gupcr_coll_parent_thread, doffset, offset,
				  sizeof (_UPC_RED_T));
		}
	      /* Wait for our value to go up the tree.  */
	      gupcr_coll_ack_wait (1);
	    }
	}
      else
	{
	  /* Avoid the case where only one thread is available.  */
	  if (!IS_ROOT_THREAD)
	    {
	      /* LEAF THREAD */
	      size_t offset = upc_addrfield ((shared void *)
					     &(gupcr_reduce_storage
					       [MYTHREAD].value[0]));
	      switch (op)
		{
		case UPC_FUNC:
		case UPC_NONCOMM_FUNC:
		  {
		    /* Schedule a triggered put once signal is received.  */
		    size_t doffset = upc_addrfield ((shared void *)
						    &(gupcr_reduce_storage
						      [MYTHREAD].
						      value
						      [gupcr_coll_child_index]));
		    gupcr_coll_trigput (gupcr_coll_parent_thread, doffset,
					offset, sizeof (_UPC_RED_T), 1);
		  }
		  break;
		default:
		  /* Schedule a triggered atomic put once parent is ready.  */
		  gupcr_coll_trigput_atomic (gupcr_coll_parent_thread, offset,
					     offset, sizeof (_UPC_RED_T),
					     gupcr_portals_reduce_op (op),
					     _UPC_TO_PTL_TYPECVT, 1);
		  break;
		}
	      /* Wait for parent to be ready.  */
	      gupcr_coll_signal_wait (1);
	      /* Wait for our value to leave.  */
	      gupcr_coll_ack_wait (1);
	    }
	}
#else /* NO TRIGGERED OPS */
      /* Send signal to all children.  */
      if (gupcr_coll_child_cnt)
	{
	  /* ROOT OR INNER THREAD */
	  int wait_cnt = gupcr_coll_child_cnt;

	  /* Signal that parent is ready to receive the locally reduced
	     values from its children. Value that we send does not matter.  */
	  for (i = 0; i < gupcr_coll_child_cnt; i++)
	    {
	      size_t offset = upc_addrfield ((shared void *)
					     &(gupcr_reduce_storage
					       [MYTHREAD].signal));
	      gupcr_coll_put (gupcr_coll_child[i], offset, offset, 1);
	    }
	  gupcr_coll_ack_wait (wait_cnt);

	  /* Wait for children to report their local reduced values and
	     parent to report it is ready to receive the reduced value.  */
	  if (!IS_ROOT_THREAD)
	    ++wait_cnt;
	  gupcr_coll_signal_wait (wait_cnt);

	  /* Compute result if reduce functions are used.  */
	  if ((op == UPC_FUNC) || (op == UPC_NONCOMM_FUNC))
	    {
	      for (i = 0; i < gupcr_coll_child_cnt; i++)
		{
		  local_result = func (local_result,
				       *(_UPC_RED_T *) &
				       gupcr_reduce_storage[MYTHREAD].value
				       [i]);
		}
	      /* Prepare reduced value for going up the tree.  */
	      *t_result = local_result;
	    }
	}
      else if (!IS_ROOT_THREAD)
	{
	  /* LEAF THREAD */
	  gupcr_coll_signal_wait (1);
	}

      /* Send reduced value to the parent.  */
      if (!IS_ROOT_THREAD)
	{
	  /* LEAF OR INNER THREAD */
	  /* Each child places its result into the parent memory slot
	     dedicated for the child. The parent is responsible
	     for creating the reduced result for itself and its
	     children.  */
	  if ((op == UPC_FUNC) || (op == UPC_NONCOMM_FUNC))
	    {
	      size_t doffset = upc_addrfield ((shared void *)
					      &(gupcr_reduce_storage
						[MYTHREAD].value
						[gupcr_coll_child_index]));
	      size_t soffset =
		upc_addrfield ((shared void *)
			       &(gupcr_reduce_storage[MYTHREAD].value[0]));
	      gupcr_coll_put (gupcr_coll_parent_thread, doffset, soffset,
			      sizeof (_UPC_RED_T));
	    }
	  else
	    {
	      size_t offset = upc_addrfield ((shared void *)
					     &(gupcr_reduce_storage
					       [MYTHREAD].value[0]));
	      gupcr_coll_put_atomic (gupcr_coll_parent_thread, offset, offset,
				     sizeof (_UPC_RED_T),
				     gupcr_portals_reduce_op (op),
				     _UPC_TO_PTL_TYPECVT);
	    }
	  gupcr_coll_ack_wait (1);
	}
#endif /* GUPCR_USE_PORTALS4_TRIGGERED_OPS */

      /* Copy result into the caller's specified destination.  */
      if (IS_ROOT_THREAD)
	{
	  *(shared _UPC_RED_T *) dst = *t_result;
	}
    }

  /* Synchronize using barriers in the cases of MYSYNC and ALLSYNC.  */
  if (UPC_OUT_MYSYNC & sync_mode || !(UPC_OUT_NOSYNC & sync_mode))
    upc_barrier;

  gupcr_trace (FC_COLL, "COLL ALL_REDUCE EXIT");
}
